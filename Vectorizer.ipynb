{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import underthesea as uts\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thích đánh lộn không? __eou__ Ngon nhà vô __eou__\\n', 'Solo yasua không __eou__ Chấp lun 2 mạng đầu __eou__\\n', 'Mai đi picnic không? __eou__ Mai bận học rồi __eou__\\n', 'Mai học ca mấy vậy? __eou__ Mai học ca 3 __eou__\\n', 'Còn tiền không? __eou__ Còn chết liền __eou__\\n']\n",
      "5855\n"
     ]
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "datapath = glob.glob(os.path.join(path,\"data/*.txt\"))\n",
    "fulldata = []\n",
    "for i in datapath:\n",
    "    with open(i,'r',encoding='utf-8') as f:\n",
    "#     lines = [i.strip() for i in f.readlines()]\n",
    "        lines = f.readlines()\n",
    "    for j in lines:\n",
    "        fulldata.append(j)\n",
    "\n",
    "print(fulldata[:5])\n",
    "print(len(fulldata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Thích đánh lộn không? ', ' Ngon nhà vô ', '\\n'], ['Solo yasua không ', ' Chấp lun 2 mạng đầu ', '\\n'], ['Mai đi picnic không? ', ' Mai bận học rồi ', '\\n'], ['Mai học ca mấy vậy? ', ' Mai học ca 3 ', '\\n'], ['Còn tiền không? ', ' Còn chết liền ', '\\n']]\n",
      "['Thích đánh lộn không? ', 'Solo yasua không ', 'Mai đi picnic không? ', 'Mai học ca mấy vậy? ', 'Còn tiền không? ']\n"
     ]
    }
   ],
   "source": [
    "data = [i.split('__eou__') for i in fulldata]\n",
    "print(data[:5])\n",
    "data_question = [i[0] for i in data]\n",
    "data_answer = [i[1] for i in data]\n",
    "print(data_question[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                question                 answer\n",
      "0                 Thích đánh lộn không?            Ngon nhà vô \n",
      "1                      Solo yasua không    Chấp lun 2 mạng đầu \n",
      "2                  Mai đi picnic không?        Mai bận học rồi \n",
      "3                   Mai học ca mấy vậy?           Mai học ca 3 \n",
      "4                       Còn tiền không?          Còn chết liền \n",
      "...                                  ...                    ...\n",
      "5850                     nhà bạn ở đâu?             Ở Cao lãnh \n",
      "5851  cho mình hỏi bạn sống ở quận nào?       Tui ở quận 8 ông \n",
      "5852               hiện bạn đang ở đâu?        Tp. Hồ Chí Minh \n",
      "5853      địa chỉ cụ thể nhà bạn là gì?     143 Nguyễn Thị Tần \n",
      "5854               nhà bạn ở đường nào?      Mới nói ở trên đó \n",
      "\n",
      "[5855 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data_table = pd.DataFrame(list(zip(data_question,data_answer)),columns={'question','answer'})\n",
    "print(data_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = ['bị','bởi','cả','các','cái','cần','càng','chỉ','chiếc','cho','chứ','chưa','có','có_thể','cứ','cùng','cũng','đã','đang','để','do','đó','được','gì','khi','không','là','lại','lên','lúc','mà','mỗi','này','nên','nếu','ngay','nhiều','như','nhưng','những','nơi','nữa','phải','qua','ra','rằng','rất','rồi','sau','sẽ','theo','thì','từ','từng','và','vẫn','vào','vậy','vì','việc','với']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer(document):\n",
    "    words = uts.word_tokenize(document)\n",
    "    \n",
    "    pos_tags = uts.pos_tag(words)\n",
    "    \n",
    "#     non_stopwords = [w for w in pos_tags if not w[0].lower() in stopwords_list]\n",
    "    \n",
    "#     non_punctuation = [w for w in non_stopwords if not w[0] in string.punctuation]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5855, 2474)\n",
      "(5855, 2474)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=uts.word_tokenize) #Still not use tokenizer\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data_question)\n",
    "count_vectorizer = CountVectorizer(tokenizer=uts.word_tokenize) #Still not use tokenizer\n",
    "count_matrix = count_vectorizer.fit_transform(data_question)\n",
    "\n",
    "print(tfidf_matrix.shape)\n",
    "print(count_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question):\n",
    "    query_vect = tfidf_vectorizer.transform([question])\n",
    "    similarity = cosine_similarity(query_vect, tfidf_matrix)\n",
    "    max_similarity = np.argmax(similarity, axis=None)\n",
    "    return data_answer[max_similarity]\n",
    "\n",
    "def ask_question_count(question):\n",
    "    query_vect = count_vectorizer.transform([question])\n",
    "    similarity = cosine_similarity(query_vect, count_matrix)\n",
    "    max_similarity = np.argmax(similarity, axis=None)a\n",
    "    return data_answer[max_similarity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF Vectorizer:  Khoảng 10 năm \n",
      "Count Vectorizer:  Khoảng 10 năm \n"
     ]
    }
   ],
   "source": [
    "print(\"TFIDF Vectorizer: \"+ ask_question('Crush bao lâu rồi?'))\n",
    "print(\"Count Vectorizer: \"+ ask_question_count('Crush bao lâu rồi?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Ngon nhà vô ', ' Chấp lun 2 mạng đầu ', ' Mai bận học rồi ', ' Mai học ca 3 ', ' Còn chết liền ', ' 11:00 rồi ', ' 1 tháng rồi ', ' Mình có rồi ', ' Khoảng 10 năm ', ' Có rồi huhu ']\n",
      "[' Ngon nhà vô ', ' Chấp lun 2 mạng đầu ', ' Mai bận học rồi ', ' Mai học ca 3 ', ' Còn chết liền ', ' 11:00 rồi ', ' 1 tháng rồi ', ' Mình có rồi ', ' Khoảng 10 năm ', ' Có rồi huhu ']\n"
     ]
    }
   ],
   "source": [
    "predict = [ask_question(i) for i in data_table['question']]\n",
    "print(predict[:10])\n",
    "predict_count = [ask_question_count(i) for i in data_table['question']]\n",
    "print(predict_count[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 question                 answer          predict-tfidf  \\\n",
      "0  Thích đánh lộn không?            Ngon nhà vô            Ngon nhà vô    \n",
      "1       Solo yasua không    Chấp lun 2 mạng đầu    Chấp lun 2 mạng đầu    \n",
      "2   Mai đi picnic không?        Mai bận học rồi        Mai bận học rồi    \n",
      "3    Mai học ca mấy vậy?           Mai học ca 3           Mai học ca 3    \n",
      "4        Còn tiền không?          Còn chết liền          Còn chết liền    \n",
      "\n",
      "           predict-count  \n",
      "0           Ngon nhà vô   \n",
      "1   Chấp lun 2 mạng đầu   \n",
      "2       Mai bận học rồi   \n",
      "3          Mai học ca 3   \n",
      "4         Còn chết liền   \n"
     ]
    }
   ],
   "source": [
    "data_table['predict-tfidf'] = predict\n",
    "data_table['predict-count'] = predict_count\n",
    "print(data_table.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8278394534585825\n",
      "0.8278394534585825\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(data_table['predict-tfidf'],data_table['answer']))\n",
    "print(accuracy_score(data_table['predict-count'],data_table['answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dạ còn chị. \n"
     ]
    }
   ],
   "source": [
    "print(ask_question('còn bao lâu nữa?'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
